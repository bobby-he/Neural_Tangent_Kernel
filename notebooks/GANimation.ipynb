{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANimation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobby-he/Neural_Tangent_Kernel/blob/master/notebooks/GANimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5eJriTUOuGvQ",
        "colab_type": "code",
        "outputId": "875722d2-8e7a-473e-aef2-a0fcb04a4964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy.special import logit, expit\n",
        "!git clone https://github.com/bobby-he/Neural_Tangent_Kernel.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Neural_Tangent_Kernel' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6mHqig1SuUeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Neural_Tangent_Kernel.src.NTK_net import LinearNeuralTangentKernel, FourLayersNet, train_net, circle_transform, variance_est, cpu_tuple,\\\n",
        "                                              AnimationPlot_lsq, kernel_leastsq_update, kernel_mats, kernel_mats_d_gan\n",
        "import time\n",
        "import copy\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Set2.colors)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c51dQjmv6HBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# normal target density\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (n, 1)))  # Gaussian\n",
        "\n",
        "# uniform latent input for generator  \n",
        "def get_generator_input_sampler():\n",
        "    return lambda n: 2*np.pi * torch.rand(n, 1)- np.pi  # Uniform-dist data into generator, _NOT_ Gaussian\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LjJWcNHl6I8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2 hidden layer Generator and Discriminator, with NTK scaling as in (Jacot et al. 2018)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1, beta = 0.1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(5), beta = 0.1)\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(5), beta = 0.1)\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map2(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map3(x)\n",
        "        #x = self.f(x)\n",
        "        #x = self.map4(x)\n",
        "        return x\n",
        "      \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(10))\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(10))\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(self.map1(x))\n",
        "        x = self.f(self.map2(x))\n",
        "        x = self.map3(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfTDPNfQvBl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GANimation(object):\n",
        "  def __init__(self, generator, discriminator, line_tuple, fig, ax, g_learning_rate = 0.001, d_learning_rate = 0.001,\n",
        "               momentum = 0.9, minibatch_size = 10, dis_iterations = 5, g_iterations = 1, n_pts = 100,\n",
        "               print_every = 50, epochs_per_frame = 100, data_mean = 4, data_stddev = 1.25, noise_prop = 0.05):\n",
        "    # Assume CUDA is available\n",
        "    self.d_learning_rate = d_learning_rate\n",
        "    self.g_learning_rate = g_learning_rate\n",
        "    \n",
        "    # define G and D and respective optimisers\n",
        "    self.G = generator\n",
        "    self.G_opt = optim.SGD(self.G.parameters(), lr=g_learning_rate, momentum=momentum)\n",
        "    self.D = discriminator\n",
        "    self.D_opt = optim.SGD(self.D.parameters(), lr=d_learning_rate, momentum=momentum)\n",
        "    \n",
        "    \n",
        "    # number of times D and G are trained at each epoch\n",
        "    self.dis_iterations = dis_iterations\n",
        "    self.g_iterations = g_iterations\n",
        "    \n",
        "    self.minibatch_size = minibatch_size\n",
        "    self.data_mean = data_mean\n",
        "    self.data_stddev = data_stddev\n",
        "    \n",
        "    # set up properties for plot output\n",
        "    self.g_backprop_line, self.d_backprop_line, self.g_kernel_line, self.d_kernel_line  = line_tuple \n",
        "    self.ax = ax\n",
        "    self.fig = fig\n",
        "    self.n_pts = n_pts\n",
        "    self.g_test_data = torch.tensor(np.linspace(-np.pi, np.pi, n_pts)).float()\n",
        "    self.d_test_data = torch.tensor(np.linspace(data_mean - 4*data_stddev, \n",
        "                                                data_mean + 4 *data_stddev, n_pts)).float().reshape((self.n_pts,1))\n",
        "    self.print_every = print_every\n",
        "    self.epochs_per_frame = epochs_per_frame\n",
        "    \n",
        "    self.criterion = nn.BCELoss() \n",
        "\n",
        "    self.g_lr = g_learning_rate\n",
        "    self.d_lr = d_learning_rate\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    \n",
        "    self.G = self.G.cuda()\n",
        "    self.D = self.D.cuda()\n",
        "    \n",
        "    self.d_sampler = get_distribution_sampler(self.data_mean, self.data_stddev)\n",
        "    self.g_sampler = get_generator_input_sampler()\n",
        "    \n",
        "    # NTK approximation for generator\n",
        "    self.g_test_circle = circle_transform(self.g_test_data).cuda()\n",
        "    self.g_kernel_output = self.G(self.g_test_circle).cpu().detach().numpy()\n",
        "    self.g_prev_kernel_output = self.G(self.g_test_circle).cpu().detach().numpy()\n",
        "    \n",
        "    # save G and D at initialisation\n",
        "    self.G_copy = copy.deepcopy(self.G)\n",
        "    self.D_copy = copy.deepcopy(self.D)\n",
        "    \n",
        "    # NTK approximation for discriminator\n",
        "    self.d_kernel_output = self.D_copy(self.d_test_data.cuda()).cpu().detach().numpy()\n",
        "    self.d_prev_kernel_output = self.d_kernel_output\n",
        "    \n",
        "  def _generator_train_step(self, fake_data, noisy_labels):\n",
        "    \n",
        "    self.G_opt.zero_grad()\n",
        "    \n",
        "    # assume noise already added to labels\n",
        "    noisy_labels = torch.tensor(noisy_labels)\n",
        "   \n",
        "    d_fake = torch.sigmoid(self.D(fake_data))\n",
        "\n",
        "    # calculate loss and optimise\n",
        "    g_loss = self.criterion(d_fake, noisy_labels.cuda())\n",
        "    g_loss.backward(retain_graph = True)\n",
        "    \n",
        "    self.G_opt.step()\n",
        "    \n",
        "    return d_fake\n",
        "  \n",
        "  def _discriminator_train_step(self, real_data, fake_data, noisy_real_labels, noisy_fake_labels):\n",
        "    noisy_real_labels = torch.tensor(noisy_real_labels)\n",
        "    noisy_fake_labels = torch.tensor(noisy_fake_labels)\n",
        "    \n",
        "    self.D_opt.zero_grad()\n",
        "    \n",
        "    d_fake = torch.sigmoid(self.D(fake_data))\n",
        "    d_real = torch.sigmoid(self.D(real_data))\n",
        "\n",
        "    # calculate loss and optimise\n",
        "    real_loss = self.criterion(d_real, noisy_real_labels.cuda())\n",
        "    fake_loss = self.criterion(d_fake, noisy_fake_labels.cuda())\n",
        "    d_loss = real_loss + fake_loss\n",
        "\n",
        "    d_loss.backward()\n",
        "    self.D_opt.step()\n",
        "    \n",
        "    return d_fake.cpu().detach().numpy(), d_real.cpu().detach().numpy()\n",
        "\n",
        "  def plot_train_step(self, i):\n",
        "    # j  will be epoch number shown in animation\n",
        "    j = 0\n",
        "    \n",
        "    if i%10==0 and i!=0:\n",
        "      print('{} steps gone'.format(i) )\n",
        "    \n",
        "    if i>2:\n",
        "      for epoch in range(self.epochs_per_frame):\n",
        "        \n",
        "        # update D first self.dis_iteration times\n",
        "        for dis_update in range(self.dis_iterations):\n",
        "          \n",
        "          # get real data and fake data batches\n",
        "          real_data = self.d_sampler(self.minibatch_size).cuda()\n",
        "          gen_samples = self.g_sampler(self.minibatch_size)\n",
        "          fake_data = self.G(circle_transform(gen_samples).reshape(self.minibatch_size, 2).cuda())\n",
        "          \n",
        "          # soft labels and random label flipping\n",
        "          noisy_real_labels = np.random.uniform(low = 0, high = 0.1, size = (self.minibatch_size,1)).astype(np.float32)\n",
        "          noisy_fake_labels = 1 - np.random.uniform(low = 0, high = 0.1, size = (self.minibatch_size,1)).astype(np.float32)\n",
        "          flipped_idx = np.random.choice(np.arange(self.minibatch_size), size=1)\n",
        "          noisy_real_labels[flipped_idx] = 1 - noisy_real_labels[flipped_idx]\n",
        "          noisy_fake_labels[flipped_idx] = 1 - noisy_fake_labels[flipped_idx]\n",
        "          \n",
        "          # update discriminator and store values D gave on real and fake\n",
        "          d_fake, d_real = self._discriminator_train_step(real_data, fake_data, noisy_real_labels, noisy_fake_labels)\n",
        "          \n",
        "          # calculate kernel weights for test data from training data. NB it is more efficient to update the weights in linearised network \n",
        "          real_k_testvtrain = kernel_mats_d_gan(self.D_copy, real_data, self.d_test_data, use_cuda = True, kernels='testvtrain').cpu().detach().numpy()\n",
        "          fake_k_testvtrain = kernel_mats_d_gan(self.D_copy, fake_data, self.d_test_data, use_cuda = True, kernels='testvtrain').cpu().detach().numpy()\n",
        "          \n",
        "          # now apply kernel d update, first defining a temporary vector that becomes self.d_kernel_output\n",
        "          temp = self.d_kernel_output + self.d_learning_rate *real_k_testvtrain @ (noisy_real_labels*(1-(d_real)) - (1-noisy_real_labels)*(d_real))/self.minibatch_size \\\n",
        "                 + self.d_learning_rate *fake_k_testvtrain @ (noisy_fake_labels*(1-(d_fake)) - (1-noisy_fake_labels)*(d_fake))/self.minibatch_size \\\n",
        "                 + self.momentum * (self.d_kernel_output - self.d_prev_kernel_output)\n",
        "          self.d_prev_kernel_output = self.d_kernel_output\n",
        "          self.d_kernel_output = temp\n",
        "          \n",
        "        # update G self.g_iterations times          \n",
        "        for g_update in range(self.g_iterations):\n",
        "          \n",
        "          # get fake data\n",
        "          gen_samples = self.g_sampler(self.minibatch_size)\n",
        "          fake_data = self.G(circle_transform(gen_samples).reshape(self.minibatch_size, 2).cuda())\n",
        "          \n",
        "          # soft labels and random label flipping\n",
        "          noisy_labels = np.random.uniform(low = 0, high = 0.1, size = (self.minibatch_size,1)).astype(np.float32)\n",
        "          flipped_idx = np.random.choice(np.arange(self.minibatch_size), size=1)\n",
        "          noisy_labels[flipped_idx] = 1 - noisy_labels[flipped_idx]\n",
        "          \n",
        "          # update G and store values D gave on fake data\n",
        "          d_fake = self._generator_train_step(fake_data, noisy_labels)\n",
        "          \n",
        "          # apply kernel g update\n",
        "          loss = sum(d_fake)/self.minibatch_size\n",
        "          d_fake = d_fake.cpu().detach().numpy()\n",
        "          fake_k_testvtrain = kernel_mats(self.G_copy, gen_samples,  self.g_test_data, n_train = self.minibatch_size, kernels = 'testvtrain').cpu().detach().numpy()\n",
        "          d_prime_vec = torch.autograd.grad(loss,fake_data, only_inputs=True, retain_graph=True)[0].cpu().numpy() # vector of derivatives of D wrt each fake output of G\n",
        "          temp = self.g_kernel_output + self.g_learning_rate*fake_k_testvtrain \\\n",
        "                 @ (d_prime_vec * (noisy_labels/d_fake - (1-noisy_labels)/(1-d_fake)))\\\n",
        "                 + self.momentum * (self.g_kernel_output - self.g_prev_kernel_output)\n",
        "          self.g_prev_kernel_output = self.g_kernel_output\n",
        "          self.g_kernel_output = temp\n",
        "          \n",
        "      j = i-2\n",
        "\n",
        "    self.fig.suptitle('Epoch {}'.format(self.epochs_per_frame * j))\n",
        "    g_current = self.G(circle_transform(self.g_test_data).cuda()).cpu().detach().numpy() \n",
        "    self.g_backprop_line.set_data(self.g_test_data.numpy(), g_current)\n",
        "    self.d_backprop_line.set_data(self.d_test_data.numpy(), torch.sigmoid(self.D(self.d_test_data.cuda())).cpu().detach().numpy())\n",
        "    \n",
        "    self.g_kernel_line.set_data(self.g_test_data.numpy(), self.g_kernel_output)\n",
        "    self.d_kernel_line.set_data(self.d_test_data.numpy(), expit(self.d_kernel_output))\n",
        "    \n",
        "    # prevent warning sign due to problem with histogram bins\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    \n",
        "    # backprop histogram\n",
        "    self.ax[1,0].clear()\n",
        "    self.ax[1,0].set_title('Approx Backprop Histogram')\n",
        "    self.ax[1,0].set_ylim((0,0.8))\n",
        "    self.ax[1,0].set_xlim((self.data_mean - 4 * self.data_stddev,self.data_mean + 4 * self.data_stddev))\n",
        "    self.ax[1,0].hist(g_current, bins = np.linspace(data_mean - 3 * data_stddev, data_mean + 3 * data_stddev, 20), density = True)\n",
        "    x = np.linspace(self.data_mean - 3*self.data_stddev, self.data_mean + 3*self.data_stddev, 100)\n",
        "    self.ax[1,0].plot(x, norm.pdf(x,self.data_mean, self.data_stddev), alpha = 0.7, color = 'c', label = 'True density')\n",
        "    self.ax[1,0].legend()\n",
        "    \n",
        "    # kernel histogram\n",
        "    self.ax[1,1].clear()\n",
        "    self.ax[1,1].set_title('Approx Kernel Histogram')\n",
        "    self.ax[1,1].set_ylim((0,0.8))\n",
        "    self.ax[1,1].set_xlim((self.data_mean - 4 * self.data_stddev,self.data_mean + 4 * self.data_stddev))\n",
        "    self.ax[1,1].hist(self.g_kernel_output, bins = np.linspace(data_mean - 3 * data_stddev, data_mean + 3 * data_stddev, 20), density = True)\n",
        "    self.ax[1,1].plot(x, norm.pdf(x,self.data_mean, self.data_stddev), alpha = 0.7, color = 'c', label = 'True density')\n",
        "    self.ax[1,1].legend()\n",
        "\n",
        "    # return plot lines for animation\n",
        "    return(self.g_backprop_line, self.d_backprop_line, self.g_kernel_line, self.d_kernel_line, )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MNH_6Xe8usxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters of target normal density\n",
        "data_mean = 4\n",
        "data_stddev = 1.25\n",
        "\n",
        "# width of NNs\n",
        "g_hidden_size = 500\n",
        "d_hidden_size = 500      \n",
        "\n",
        "generator_activation_function = F.leaky_relu\n",
        "discriminator_activation_function = torch.sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f4ZD7JTw_7KO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (13,13))\n",
        "plt.subplots_adjust(wspace=0.15,hspace=0.15)\n",
        "plt.close()\n",
        "\n",
        "ax[0,0].set_xlim((-np.pi, np.pi))\n",
        "ax[0,0].set_ylim((data_mean - 4 * data_stddev, data_mean + 4 * data_stddev))\n",
        "ax[0,1].set_xlim((data_mean - 4 * data_stddev, data_mean + 4 * data_stddev))\n",
        "ax[0,1].set_ylim((0,1))\n",
        "ax[0,0].set_xlabel('$z$')\n",
        "ax[0,0].set_ylabel('$G_{ \\\\theta}(z)$')\n",
        "ax[0,1].set_xlabel('$x$')\n",
        "ax[0,1].set_ylabel('$D_{ \\phi}(x)$')\n",
        "ax[0,0].set_title('Generator')\n",
        "ax[0,1].set_title('Discriminator')\n",
        "ax[1,0].set_ylabel('Density')\n",
        "ax[1,1].set_ylabel('Density')\n",
        "ax[0,1].axhline(0.5, linestyle='--', color = 'darkgrey', alpha = 0.9, linewidth = 1)\n",
        "\n",
        "line0, = ax[0,0].plot([], [], lw=1, linestyle = '--', color = 'darkmagenta', label = 'Backprop')\n",
        "line0a, = ax[0,0].plot([], [], lw=1, color = 'darkmagenta', label = 'Kernel GD')\n",
        "line1, = ax[0,1].plot([], [], lw=1, color = 'r', linestyle = '--', label = 'Backprop')\n",
        "line1a, = ax[0,1].plot([], [], lw=1, color = 'r', label = 'Kernel GD')\n",
        "\n",
        "line_tuple = (line0, line1, line0a, line1a)\n",
        "\n",
        "ax[0,0].legend(loc = 'upper left')\n",
        "ax[0,1].legend(loc = 'upper left')\n",
        "\n",
        "G = Generator(input_size = 2,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size = 1,\n",
        "                  f=generator_activation_function).cuda()   \n",
        "D = Discriminator(input_size = 1,\n",
        "                  hidden_size=d_hidden_size,\n",
        "                  output_size = 1,\n",
        "                  f=discriminator_activation_function).cuda()\n",
        "\n",
        "# instantiate object\n",
        "GAN = GANimation(G,D,data_mean = data_mean, data_stddev = data_stddev, dis_iterations=10, line_tuple = line_tuple, ax = ax, fig = fig, epochs_per_frame=10)\n",
        "\n",
        "start=time.time() # this should take 6 hours on colab GPU. More efficient to apply linearised weight updates in parameter space.\n",
        "anim = animation.FuncAnimation(fig, GAN.plot_train_step, frames = 150, interval = 150, blit = True)\n",
        "rc('animation', html='jshtml')\n",
        "anim.save('anim_gan_good.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jeh3FN6ZPCs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "end = time.time() - start"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBXBlDaYPdgA",
        "colab_type": "code",
        "outputId": "770f4d50-e0fc-4117-f8e1-da4e8d06bf3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.37035846710205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}