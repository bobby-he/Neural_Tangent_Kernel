{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ganredemption.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobby-he/Neural_Tangent_Kernel/blob/master/notebooks/GANimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5eJriTUOuGvQ",
        "colab_type": "code",
        "outputId": "875722d2-8e7a-473e-aef2-a0fcb04a4964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from scipy.special import logit, expit\n",
        "!git clone https://github.com/bobby-he/Neural_Tangent_Kernel.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Neural_Tangent_Kernel' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6mHqig1SuUeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from Neural_Tangent_Kernel.src.NTK_net import LinearNeuralTangentKernel, FourLayersNet, train_net, circle_transform, variance_est, cpu_tuple,\\\n",
        "                                              AnimationPlot_lsq, kernel_leastsq_update, kernel_mats, kernel_mats_d_gan\n",
        "import time\n",
        "import copy\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Set2.colors)\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (n, 1)))  # Gaussian\n",
        "\n",
        "def get_generator_input_sampler():\n",
        "    return lambda n: 2*np.pi * torch.rand(n, 1)- np.pi  # Uniform-dist data into generator, _NOT_ Gaussian\n",
        "\n",
        "#def get_generator_input_sampler():\n",
        "  #return lambda n: torch.Tensor(np.random.normal(0, 1, (n, 1)))  # Uniform-dist data into generator, _NOT_ Gaussian\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1, beta = 0.1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(5), beta = 0.1)\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(5), beta = 0.1)\n",
        "        #self.map4 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(5), beta = 0.1)\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map2(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map3(x)\n",
        "        #x = self.f(x)\n",
        "        #x = self.map4(x)\n",
        "        return x\n",
        "      \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(10))\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(10))\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(self.map1(x))\n",
        "        x = self.f(self.map2(x))\n",
        "        x = self.map3(x)\n",
        "        return x\n",
        "      \n",
        "class Discriminator_no_sig(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator_no_sig, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(10))\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(10))\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(self.map1(x))\n",
        "        x = self.f(self.map2(x))\n",
        "        x = self.map3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfTDPNfQvBl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GANimation(object):\n",
        "  def __init__(self, generator, discriminator, line_tuple, fig, ax, g_learning_rate = 0.001, d_learning_rate = 0.001,\n",
        "               momentum = 0.9, minibatch_size = 10, dis_iterations = 5, g_iterations = 1, n_pts = 100,\n",
        "               print_every = 50, epochs_per_frame = 100, data_mean = 4, data_stddev = 1.25, noise_prop = 0.05):\n",
        "    # Assume CUDA is available\n",
        "    self.d_learning_rate = d_learning_rate\n",
        "    self.g_learning_rate = g_learning_rate\n",
        "    self.G = generator\n",
        "    self.G_opt = optim.SGD(self.G.parameters(), lr=g_learning_rate, momentum=momentum)\n",
        "    self.D = discriminator\n",
        "    self.D_opt = optim.SGD(self.D.parameters(), lr=d_learning_rate, momentum=momentum)\n",
        "    self.dis_iterations = dis_iterations\n",
        "    self.g_iterations = g_iterations\n",
        "    self.minibatch_size = minibatch_size\n",
        "    self.data_mean = data_mean\n",
        "    self.data_stddev = data_stddev\n",
        "    self.g_backprop_line, self.d_backprop_line, self.g_kernel_line, self.d_kernel_line  = line_tuple #self.d_kernel_line\n",
        "    self.ax = ax\n",
        "    self.fig = fig\n",
        "    self.n_pts = n_pts\n",
        "    self.g_test_data = torch.tensor(np.linspace(-np.pi, np.pi, n_pts)).float()\n",
        "    self.d_test_data = torch.tensor(np.linspace(data_mean - 4*data_stddev, \n",
        "                                                data_mean + 4 *data_stddev, n_pts)).float().reshape((self.n_pts,1))\n",
        "    self.print_every = print_every\n",
        "    self.epochs_per_frame = epochs_per_frame\n",
        "    \n",
        "    self.criterion = nn.BCELoss() \n",
        "    self.valid = Variable(torch.ones([minibatch_size,1]), requires_grad=False).cuda()\n",
        "    self.fake = Variable(torch.zeros([minibatch_size,1]),  requires_grad=False).cuda()\n",
        "\n",
        "    self.g_lr = g_learning_rate\n",
        "    self.d_lr = d_learning_rate\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    \n",
        "    self.G.cuda()\n",
        "    self.D.cuda()\n",
        "    \n",
        "    self.d_sampler = get_distribution_sampler(self.data_mean, self.data_stddev)\n",
        "    self.g_sampler = get_generator_input_sampler()\n",
        "    \n",
        "    # and finally, the kernel approximation\n",
        "    self.g_test_circle = circle_transform(self.g_test_data).cuda()\n",
        "    self.g_kernel_output = self.G(self.g_test_circle).cpu().detach().numpy()\n",
        "    self.g_prev_kernel_output = self.G(self.g_test_circle).cpu().detach().numpy()\n",
        "    \n",
        "    self.G_copy = copy.deepcopy(self.G)\n",
        "    self.D_no_sig = copy.deepcopy(self.D)\n",
        "    \n",
        "    self.d_kernel_output = self.D_no_sig(self.d_test_data.cuda()).cpu().detach().numpy()\n",
        "    self.d_prev_kernel_output = self.d_kernel_output\n",
        "    \n",
        "  def _generator_train_step(self, fake_data, noisy_labels):\n",
        "    self.G_opt.zero_grad()\n",
        "    noisy_labels = torch.tensor(noisy_labels)\n",
        "   \n",
        "    d_fake = torch.sigmoid(self.D(fake_data))\n",
        "\n",
        "    # calculate loss and optimise\n",
        "    g_loss = self.criterion(d_fake, noisy_labels.cuda())\n",
        "    g_loss.backward(retain_graph = True)\n",
        "    self.G_opt.step()\n",
        "    \n",
        "    return d_fake\n",
        "  \n",
        "  def _discriminator_train_step(self, real_data, fake_data, noisy_real_labels, noisy_fake_labels):\n",
        "    # save factors used in label smoothing\n",
        "    noisy_real_labels = torch.tensor(noisy_real_labels)\n",
        "    noisy_fake_labels = torch.tensor(noisy_fake_labels)\n",
        "    self.D_opt.zero_grad()\n",
        "    d_fake = torch.sigmoid(self.D(fake_data))\n",
        "    d_real = torch.sigmoid(self.D(real_data))\n",
        "\n",
        "    # calculate loss and optimise\n",
        "    real_loss = self.criterion(d_real, noisy_real_labels.cuda())\n",
        "    fake_loss = self.criterion(d_fake, noisy_fake_labels.cuda())\n",
        "    d_loss = real_loss + fake_loss\n",
        "\n",
        "    d_loss.backward()\n",
        "    self.D_opt.step()\n",
        "    \n",
        "    return d_fake.cpu().detach().numpy(), d_real.cpu().detach().numpy()\n",
        "\n",
        "  def plot_train_step(self, i):\n",
        "    j = 0\n",
        "    if i%10==0 and i!=0:\n",
        "      print('{} steps gone'.format(i) )\n",
        "    if i>2:\n",
        "      for epoch in range(self.epochs_per_frame):\n",
        "        for dis_update in range(self.dis_iterations):\n",
        "          real_data = self.d_sampler(self.minibatch_size).cuda()\n",
        "          gen_samples = self.g_sampler(self.minibatch_size)\n",
        "          fake_data = self.G(circle_transform(gen_samples).reshape(self.minibatch_size, 2).cuda())\n",
        "          noisy_real_labels = np.random.uniform(low = 0, high = 0.1, size = (self.minibatch_size,1)).astype(np.float32)\n",
        "          noisy_fake_labels = 1 - np.random.uniform(low = 0, high = 0.1, size = (self.minibatch_size,1)).astype(np.float32)\n",
        "          flipped_idx = np.random.choice(np.arange(self.minibatch_size), size=1)\n",
        "          noisy_real_labels[flipped_idx] = 1 - noisy_real_labels[flipped_idx]\n",
        "          noisy_fake_labels[flipped_idx] = 1 - noisy_fake_labels[flipped_idx]\n",
        "          d_fake, d_real = self._discriminator_train_step(real_data, fake_data, noisy_real_labels, noisy_fake_labels)\n",
        "          \n",
        "          real_k_testvtrain = kernel_mats_d_gan(self.D_no_sig, real_data, self.d_test_data, use_cuda = True, kernels='testvtrain').cpu().detach().numpy()\n",
        "          fake_k_testvtrain = kernel_mats_d_gan(self.D_no_sig, fake_data, self.d_test_data, use_cuda = True, kernels='testvtrain').cpu().detach().numpy()\n",
        "          \n",
        "          # now apply kernel update, first defining a temporary vector that becomes self.d_kernel_output\n",
        "          temp = self.d_kernel_output + self.d_learning_rate *real_k_testvtrain @ (noisy_real_labels*(1-(d_real)) - (1-noisy_real_labels)*(d_real))/self.minibatch_size \\\n",
        "                 + self.d_learning_rate *fake_k_testvtrain @ (noisy_fake_labels*(1-(d_fake)) - (1-noisy_fake_labels)*(d_fake))/self.minibatch_size \\\n",
        "                 + self.momentum * (self.d_kernel_output - self.d_prev_kernel_output)\n",
        "          \n",
        "          self.d_prev_kernel_output = self.d_kernel_output\n",
        "          self.d_kernel_output = temp\n",
        "          \n",
        "        for g_update in range(self.g_iterations):\n",
        "          gen_samples = self.g_sampler(self.minibatch_size)\n",
        "          fake_data = self.G(circle_transform(gen_samples).reshape(self.minibatch_size, 2).cuda())\n",
        "          noisy_labels = np.random.uniform(low = 0, high = 0.1, size = (self.minibatch_size,1)).astype(np.float32)\n",
        "          flipped_idx = np.random.choice(np.arange(self.minibatch_size), size=1)\n",
        "          noisy_labels[flipped_idx] = 1 - noisy_labels[flipped_idx]\n",
        "          d_fake = self._generator_train_step(fake_data, noisy_labels)\n",
        "          \n",
        "          loss = sum(d_fake)/self.minibatch_size\n",
        "          d_fake = d_fake.cpu().detach().numpy()\n",
        "          fake_k_testvtrain = kernel_mats(self.G_copy, gen_samples,  self.g_test_data, n_train = self.minibatch_size, kernels = 'testvtrain').cpu().detach().numpy()\n",
        "          d_prime_vec = torch.autograd.grad(loss,fake_data, only_inputs=True, retain_graph=True)[0].cpu().numpy() # vector of derivatives of D wrt each fake output of G\n",
        "          temp = self.g_kernel_output + self.g_learning_rate*fake_k_testvtrain \\\n",
        "                 @ (d_prime_vec * (noisy_labels/d_fake - (1-noisy_labels)/(1-d_fake)))\\\n",
        "                 + self.momentum * (self.g_kernel_output - self.g_prev_kernel_output)\n",
        "          self.g_prev_kernel_output = self.g_kernel_output\n",
        "          self.g_kernel_output = temp\n",
        "          \n",
        "      j = i-2\n",
        "\n",
        "    self.fig.suptitle('Epoch {}'.format(self.epochs_per_frame *j))\n",
        "    g_current = self.G(circle_transform(self.g_test_data).cuda()).cpu().detach().numpy() \n",
        "    self.g_backprop_line.set_data(self.g_test_data.numpy(), g_current)\n",
        "    self.d_backprop_line.set_data(self.d_test_data.numpy(), torch.sigmoid(self.D(self.d_test_data.cuda())).cpu().detach().numpy())\n",
        "    \n",
        "    self.g_kernel_line.set_data(self.g_test_data.numpy(), self.g_kernel_output)\n",
        "    self.d_kernel_line.set_data(self.d_test_data.numpy(), expit(self.d_kernel_output))\n",
        "    \n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    \n",
        "    self.ax[1,0].clear()\n",
        "    self.ax[1,0].set_title('Approx Backprop Histogram')\n",
        "    self.ax[1,0].set_ylim((0,0.8))\n",
        "    self.ax[1,0].set_xlim((self.data_mean - 4 * self.data_stddev,self.data_mean + 4 * self.data_stddev))\n",
        "    self.ax[1,0].hist(g_current, bins = np.linspace(data_mean - 3 * data_stddev, data_mean + 3 * data_stddev, 20), density = True)\n",
        "    x = np.linspace(self.data_mean - 3*self.data_stddev, self.data_mean + 3*self.data_stddev, 100)\n",
        "    self.ax[1,0].plot(x, norm.pdf(x,self.data_mean, self.data_stddev), alpha = 0.7, color = 'c', label = 'True density')\n",
        "    self.ax[1,0].legend()\n",
        "    \n",
        "    self.ax[1,1].clear()\n",
        "    self.ax[1,1].set_title('Approx Kernel Histogram')\n",
        "    self.ax[1,1].set_ylim((0,0.8))\n",
        "    self.ax[1,1].set_xlim((self.data_mean - 4 * self.data_stddev,self.data_mean + 4 * self.data_stddev))\n",
        "    self.ax[1,1].hist(self.g_kernel_output, bins = np.linspace(data_mean - 3 * data_stddev, data_mean + 3 * data_stddev, 20), density = True)\n",
        "    self.ax[1,1].plot(x, norm.pdf(x,self.data_mean, self.data_stddev), alpha = 0.7, color = 'c', label = 'True density')\n",
        "    self.ax[1,1].legend()\n",
        "\n",
        "\n",
        "    return(self.g_backprop_line, self.d_backprop_line, self.g_kernel_line, self.d_kernel_line, )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MNH_6Xe8usxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_mean = 5\n",
        "data_stddev = 1.25\n",
        "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (13,13))\n",
        "plt.subplots_adjust(wspace=0.15,hspace=0.15)\n",
        "plt.close()\n",
        "\n",
        "ax[0,0].set_xlim((-np.pi, np.pi))\n",
        "ax[0,0].set_ylim((data_mean - 4 * data_stddev, data_mean + 4 * data_stddev))\n",
        "ax[0,1].set_xlim((data_mean - 4 * data_stddev, data_mean + 4 * data_stddev))\n",
        "ax[0,1].set_ylim((0,1))\n",
        "ax[0,0].set_xlabel('$z$')\n",
        "ax[0,0].set_ylabel('$G_{ \\\\theta}(z)$')\n",
        "ax[0,1].set_xlabel('$x$')\n",
        "ax[0,1].set_ylabel('$D_{ \\phi}(x)$')\n",
        "ax[0,0].set_title('Generator')\n",
        "ax[0,1].set_title('Discriminator')\n",
        "ax[1,0].set_ylabel('Density')\n",
        "ax[1,1].set_ylabel('Density')\n",
        "ax[0,1].axhline(0.5, linestyle='--', color = 'darkgrey', alpha = 0.9, linewidth = 1)\n",
        "\n",
        "line0, = ax[0,0].plot([], [], lw=1, linestyle = '--', color = 'darkmagenta', label = 'Backprop')\n",
        "line0a, = ax[0,0].plot([], [], lw=1, color = 'darkmagenta', label = 'Kernel GD')\n",
        "line1, = ax[0,1].plot([], [], lw=1, color = 'r', linestyle = '--', label = 'Backprop')\n",
        "line1a, = ax[0,1].plot([], [], lw=1, color = 'r', label = 'Kernel GD')\n",
        "\n",
        "#hist0 = ax[1,0].hist([], bins = np.linspace(data_mean - 2 * data_stddev, data_mean + 2 * data_stddev, 20), density = True)\n",
        "#hist1 = ax[1,0].hist([], bins = np.linspace(data_mean - 2 * data_stddev, data_mean + 2 * data_stddev, 20), density = True)\n",
        "line_tuple = (line0, line1, line0a, line1a)#, hist0, hist1)\n",
        "\n",
        "ax[0,0].legend(loc = 'upper left')\n",
        "ax[0,1].legend(loc = 'upper left')\n",
        "g_hidden_size = 500\n",
        "d_hidden_size = 500      \n",
        "generator_activation_function = torch.tanh\n",
        "discriminator_activation_function = torch.sigmoid\n",
        "G = Generator(input_size = 2,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size = 1,\n",
        "                  f=generator_activation_function).cuda()   \n",
        "D = Discriminator(input_size = 1,\n",
        "                  hidden_size=d_hidden_size,\n",
        "                  output_size = 1,\n",
        "                  f=discriminator_activation_function).cuda()\n",
        "#D = nn.Sequential(D_no_sig,nn.Sigmoid())\n",
        "GAN = GANimation(G,D,data_mean = data_mean, data_stddev = data_stddev, dis_iterations=10, line_tuple = line_tuple, ax = ax, fig = fig, epochs_per_frame=10)\n",
        "\n",
        "\n",
        "start=time.time()\n",
        "anim = animation.FuncAnimation(fig, GAN.plot_train_step, frames = 150, interval = 150, blit = True)\n",
        "rc('animation', html='jshtml')\n",
        "anim.save('anim_gan_good.mp4')\n",
        "files.download('anim_gan_good.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jeh3FN6ZPCs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "end = time.time() - start"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBXBlDaYPdgA",
        "colab_type": "code",
        "outputId": "770f4d50-e0fc-4117-f8e1-da4e8d06bf3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.37035846710205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}