{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobby-he/Neural_Tangent_Kernel/blob/master/notebooks/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SjS-N56V0g5c",
        "colab_type": "code",
        "outputId": "447452f6-b9c6-42a5-a320-38cddf1eee3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bobby-he/Neural_Tangent_Kernel.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Neural_Tangent_Kernel'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 179 (delta 74), reused 35 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (179/179), 21.47 MiB | 9.77 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "urvXNwWp0mmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Neural_Tangent_Kernel.src.NTK_net import LinearNeuralTangentKernel, FourLayersNet, train_net, circle_transform, variance_est, cpu_tuple,\\\n",
        "                                              AnimationPlot_lsq, kernel_leastsq_update, kernel_mats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDRceg-lvLg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "matplotlib_is_available = True\n",
        "try:\n",
        "  from matplotlib import pyplot as plt\n",
        "except ImportError:\n",
        "  print(\"Will skip plotting; matplotlib is not available.\")\n",
        "  matplotlib_is_available = False\n",
        "\n",
        "# Data params\n",
        "data_mean =4\n",
        "data_stddev = 1.25\n",
        "\n",
        "# ### Uncomment only one of these to define what data is actually sent to the Discriminator\n",
        "(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
        "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
        "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
        "#(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNFNwy_xH_lR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e45fb077-708d-439c-d8c4-7b8be95c8973"
      },
      "cell_type": "code",
      "source": [
        "print(\"Using data [%s]\" % (name))\n",
        "\n",
        "# ##### DATA: Target data and generator input data\n",
        "\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (n, 1)))  # Gaussian\n",
        "\n",
        "def get_generator_input_sampler():\n",
        "    return lambda n: torch.rand(n, 1)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
        "\n",
        "# ##### MODELS: Generator model and discriminator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(5))\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(5))\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map2(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map3(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = LinearNeuralTangentKernel(input_size, hidden_size, w_sig = 1)\n",
        "        self.map2 = LinearNeuralTangentKernel(hidden_size, hidden_size, w_sig = np.sqrt(10))\n",
        "        self.map3 = LinearNeuralTangentKernel(hidden_size, output_size, w_sig = np.sqrt(10))\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(self.map1(x))\n",
        "        x = self.f(self.map2(x))\n",
        "        x = self.f(self.map3(x))\n",
        "        return x\n",
        "\n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "\n",
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]\n",
        "\n",
        "def trainlol():\n",
        "    # Model parameters\n",
        "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
        "    g_hidden_size = 500     # Generator complexity\n",
        "    g_output_size = 1     # Size of generated output vector\n",
        "    d_input_size = 1  # Minibatch size - cardinality of distributions\n",
        "    d_hidden_size = 500    # Discriminator complexity\n",
        "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
        "    minibatch_size = 500\n",
        "\n",
        "    d_learning_rate = 0.001\n",
        "    g_learning_rate = 0.001\n",
        "    sgd_momentum = 0.9\n",
        "\n",
        "    num_epochs = 20000\n",
        "    print_interval = 1000\n",
        "    d_steps = 5\n",
        "    g_steps = 1\n",
        "\n",
        "    dfe, dre, ge = 0, 0, 0\n",
        "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
        "\n",
        "    discriminator_activation_function = torch.sigmoid\n",
        "    generator_activation_function = torch.tanh\n",
        "\n",
        "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "    gi_sampler = get_generator_input_sampler()\n",
        "    G = Generator(input_size=g_input_size,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size=g_output_size,\n",
        "                  f=generator_activation_function).cuda()\n",
        "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
        "                      hidden_size=d_hidden_size,\n",
        "                      output_size=d_output_size,\n",
        "                      f=discriminator_activation_function).cuda()\n",
        "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
        "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for d_index in range(d_steps):\n",
        "            # 1. Train D on real+fake\n",
        "            D.zero_grad()\n",
        "\n",
        "            #  1A: Train D on real\n",
        "            d_real_data = Variable(d_sampler(minibatch_size)).cuda()\n",
        "            d_real_decision = D(d_real_data)\n",
        "            d_real_error = criterion(d_real_decision, Variable(0.95*torch.ones([minibatch_size,1])).cuda())  # ones = true\n",
        "            d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "            #  1B: Train D on fake\n",
        "            d_gen_input = Variable(gi_sampler(minibatch_size)).cuda()\n",
        "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "            d_fake_decision = D(d_fake_data)\n",
        "            d_fake_error = criterion(d_fake_decision, Variable(0.05*torch.ones([minibatch_size,1])).cuda())  # zeros = fake\n",
        "            d_fake_error.backward()\n",
        "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
        "\n",
        "        for g_index in range(g_steps):\n",
        "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "            G.zero_grad()\n",
        "\n",
        "            gen_input = Variable(gi_sampler(minibatch_size)).cuda()\n",
        "            g_fake_data = G(gen_input)\n",
        "            dg_fake_decision = D(preprocess(g_fake_data))\n",
        "            g_error = criterion(dg_fake_decision, Variable(0.95*torch.ones([minibatch_size,1])).cuda())  # Train G to pretend it's genuine\n",
        "\n",
        "            g_error.backward()\n",
        "            g_optimizer.step()  # Only optimizes G's parameters\n",
        "            ge = extract(g_error)[0]\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(\"Epoch %s: Fake Dist (%s),  D (%s real_err, %s fake_err) G (%s err); Real Dist (%s)\" %\n",
        "                  (epoch, stats(extract(d_fake_data)), dre, dfe, ge, stats(extract(d_real_data))))\n",
        "            plt.clf()\n",
        "            values = extract(g_fake_data)\n",
        "            plt.hist(values, bins=20)\n",
        "            plt.xlabel('Value')\n",
        "            plt.ylabel('Count')\n",
        "            plt.title('Histogram of Generated Distribution')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "            \n",
        "    gen_input = Variable(gi_sampler(5000)).cuda()\n",
        "    g_fake_data = G(gen_input)\n",
        "    \n",
        "    if matplotlib_is_available:\n",
        "        print(\"Plotting the generated distribution...\")\n",
        "        values = extract(g_fake_data)\n",
        "        #print(\" Values: %s\" % (str(values)))\n",
        "        plt.clf()\n",
        "        plt.hist(values, bins=100)\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Histogram of Generated Distribution')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using data [Raw data]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FmlfCTpW-oL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
        "    g_hidden_size = 500     # Generator complexity\n",
        "    g_output_size = 1     # Size of generated output vector\n",
        "    d_input_size = 1  # Minibatch size - cardinality of distributions\n",
        "    d_hidden_size = 500    # Discriminator complexity\n",
        "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
        "    minibatch_size = 500\n",
        "\n",
        "    d_learning_rate = 0.001\n",
        "    g_learning_rate = 0.001\n",
        "    sgd_momentum = 0.9\n",
        "\n",
        "    num_epochs = 20000\n",
        "    print_interval = 1000\n",
        "    d_steps = 5\n",
        "    g_steps = 1\n",
        "\n",
        "    dfe, dre, ge = 0, 0, 0\n",
        "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
        "\n",
        "    discriminator_activation_function = torch.sigmoid\n",
        "    generator_activation_function = torch.tanh\n",
        "\n",
        "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "    gi_sampler = get_generator_input_sampler()\n",
        "    G = Generator(input_size=g_input_size,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size=g_output_size,\n",
        "                  f=generator_activation_function).cuda()\n",
        "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
        "                      hidden_size=d_hidden_size,\n",
        "                      output_size=d_output_size,\n",
        "                      f=discriminator_activation_function).cuda()\n",
        "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
        "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M__QNnzFXAdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad67123c-8b6b-492b-9f14-fa5364bad25e"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(dict, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "3RhP6o6uIASa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}