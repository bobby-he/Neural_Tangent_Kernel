{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2XFI8Wc2J5MX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Neural Tangent Kernel"
      ]
    },
    {
      "metadata": {
        "id": "YLA7_UcOM9Si",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Despite widespread practical success, the training behaviour of Deep Neural Networks (DNNs) has always been a poorly understood concept. Much focus has been placed on understanding convergence in parameter space, but it is well known that  the loss surface of DNNs are a high dimensional  and non-convex. They contain many saddle-points, valleys and symmetries that render it unreasonable to expect that a gradient based optimiser will be able to converge to the global minimum, if one exists.  In spite of this, DNNs are also known for their good generalisation properties, despite their seemingly obvious over-parametrisation. \n",
        "\n",
        "This report will study some recent papers that aim to shed light on the dyanmics of DNN training. The main idea involved is to view the training process in terms of the function $f_{\\theta}$ that the DNN represents instead of the parameters $\\theta$"
      ]
    },
    {
      "metadata": {
        "id": "Z6bF6f2J5VK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Scs43oBz5UHS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "vM0e10wPsmQq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reference list, delete before handing in\n",
        "\n",
        "[Jacot et al.,  2018](https://arxiv.org/pdf/1806.07572.pdf)\n",
        "\n",
        "[Yang, 2019](https://arxiv.org/pdf/1902.04760.pdf)\n",
        "\n",
        "[Lee et al., 2019](https://arxiv.org/pdf/1902.06720.pdf)\n",
        "\n"
      ]
    }
  ]
}